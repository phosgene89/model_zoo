{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatiotemporal Forecasting Models with Moving MNIST\n",
    "Moving MNIST is a video prediction task consisting of short, 20 frame videos of moving numbers. The task is to use the first 10 frames to predict the next 10 frames. \n",
    "\n",
    "Models currently implemented are:\n",
    "\n",
    " * ConvLSTM (neural network code has been taken from <a href=\"https://github.com/jhhuang96\">jhhuang96's</a> ConvLSTM <a href=\"https://github.com/jhhuang96/ConvLSTM-PyTorch\">GitHub repository</a>)\n",
    " \n",
    "Models to be implemented:\n",
    "\n",
    " * Physical Dynamics Net (https://arxiv.org/abs/2003.01460)\n",
    " * Scalable Gradients for Stochastic Differential Equations (https://arxiv.org/abs/2001.01328)\n",
    " * ODE2VAE: Deep generative second order ODEs with Bayesian neural networks (https://arxiv.org/abs/1905.10994)\n",
    " * Disentangling Multiple Features in Video Sequences using Gaussian Processes in Variational Autoencoders (https://arxiv.org/abs/2001.02408)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#from data.mm import MovingMNIST\n",
    "import sys\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import logging\n",
    "from ConvRNN.generate_model import *\n",
    "from earlystopping import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatiotemporalDataset(Dataset):\n",
    "\n",
    "    def __init__(self, X, forecast_horizon, lags):\n",
    "        self.X = X\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.lags = lags\n",
    "\n",
    "    def __len__(self):\n",
    "        length = len(self.X) - self.forecast_horizon - self.lags + 1\n",
    "        return length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx:idx+self.lags]\n",
    "        y = self.X[idx+self.lags:idx+self.lags+self.forecast_horizon]\n",
    "\n",
    "        return idx, x, y, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 1000\n",
    "N_CHANNELS = 1\n",
    "IM_HEIGHT = 64\n",
    "IM_WIDTH = 64\n",
    "FORECAST_HORIZON = 10\n",
    "LAGS = 10\n",
    "\n",
    "X = np.random.normal(0, 1, (N_SAMPLES, N_CHANNELS, IM_HEIGHT, IM_WIDTH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set hyperparameters and random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = \"2020-03-09T00-00-00\"\n",
    "\n",
    "batch_size = 4\n",
    "lr = 1e-4\n",
    "frames_input = 10\n",
    "frames_output = 10\n",
    "epochs = 500\n",
    "\n",
    "random_seed = 1996\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "else:\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    \n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "save_dir = './save_model/' + TIMESTAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create DataLoaders\n",
    "These will be used to feed batches of data to the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = SpatiotemporalDataset(X, FORECAST_HORIZON, LAGS)\n",
    "validDataset = SpatiotemporalDataset(X, FORECAST_HORIZON, LAGS)\n",
    "\n",
    "trainLoader = torch.utils.data.DataLoader(trainDataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "validLoader = torch.utils.data.DataLoader(validDataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get examples of the contents of the DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 10, 1, 64, 64])\n",
      "Target shape: torch.Size([4, 10, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "(idx, targetVar, inputVar, _, _) = next(iter(trainLoader))\n",
    "\n",
    "print(\"Input shape: {}\".format(np.shape(inputVar)))\n",
    "print(\"Target shape: {}\".format(np.shape(targetVar)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the ConvLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0\n"
     ]
    }
   ],
   "source": [
    "net = generate_convlstm().float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define the training and validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, device):\n",
    "    '''\n",
    "    main function to run the training\n",
    "    '''\n",
    "    run_dir = './runs/' + TIMESTAMP\n",
    "    if not os.path.isdir(run_dir):\n",
    "        os.makedirs(run_dir)\n",
    "    tb = SummaryWriter(run_dir)\n",
    "    # initialize the early_stopping object\n",
    "    early_stopping = EarlyStopping(patience=20, verbose=True)\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        net = nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "\n",
    "    if os.path.exists(os.path.join(save_dir, 'checkpoint.pth.tar')):\n",
    "        # load existing model\n",
    "        print('==> loading existing model')\n",
    "        model_info = torch.load(os.path.join(save_dir, 'checkpoin.pth.tar'))\n",
    "        net.load_state_dict(model_info['state_dict'])\n",
    "        optimizer = torch.optim.Adam(net.parameters())\n",
    "        optimizer.load_state_dict(model_info['optimizer'])\n",
    "        cur_epoch = model_info['epoch'] + 1\n",
    "    else:\n",
    "        if not os.path.isdir(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        cur_epoch = 0\n",
    "    lossfunction = nn.MSELoss().cuda()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    pla_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                      factor=0.5,\n",
    "                                                      patience=4,\n",
    "                                                      verbose=True)\n",
    "\n",
    "    # to track the training loss as the model trains\n",
    "    train_losses = []\n",
    "    # to track the validation loss as the model trains\n",
    "    valid_losses = []\n",
    "    # to track the average training loss per epoch as the model trains\n",
    "    avg_train_losses = []\n",
    "    # to track the average validation loss per epoch as the model trains\n",
    "    avg_valid_losses = []\n",
    "    # mini_val_loss = np.inf\n",
    "    for epoch in range(cur_epoch, epochs + 1):\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        t = tqdm(trainLoader, leave=False, total=len(trainLoader))\n",
    "        for i, (idx, targetVar, inputVar, _, _) in enumerate(t):\n",
    "            inputs = inputVar.to(device).float()  # B,S,C,H,W\n",
    "            label = targetVar.to(device).float()  # B,S,C,H,W\n",
    "            optimizer.zero_grad()\n",
    "            net.train()\n",
    "            pred = net(inputs)  # B,S,C,H,W\n",
    "            loss = lossfunction(pred, label)\n",
    "            loss_aver = loss.item() / batch_size\n",
    "            train_losses.append(loss_aver)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_value_(net.parameters(), clip_value=10.0)\n",
    "            optimizer.step()\n",
    "            t.set_postfix({\n",
    "                'trainloss': '{:.6f}'.format(loss_aver),\n",
    "                'epoch': '{:02d}'.format(epoch)\n",
    "            })\n",
    "        tb.add_scalar('TrainLoss', loss_aver, epoch)\n",
    "        ######################\n",
    "        # validate the model #\n",
    "        ######################\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            t = tqdm(validLoader, leave=False, total=len(validLoader))\n",
    "            for i, (idx, targetVar, inputVar, _, _) in enumerate(t):\n",
    "                if i == 3000:\n",
    "                    break\n",
    "                inputs = inputVar.to(device).float()\n",
    "                label = targetVar.to(device).float()\n",
    "                pred = net(inputs)\n",
    "                loss = lossfunction(pred, label)\n",
    "                loss_aver = loss.item() / batch_size\n",
    "                # record validation loss\n",
    "                valid_losses.append(loss_aver)\n",
    "                #print (\"validloss: {:.6f},  epoch : {:02d}\".format(loss_aver,epoch),end = '\\r', flush=True)\n",
    "                t.set_postfix({\n",
    "                    'validloss': '{:.6f}'.format(loss_aver),\n",
    "                    'epoch': '{:02d}'.format(epoch)\n",
    "                })\n",
    "\n",
    "        tb.add_scalar('ValidLoss', loss_aver, epoch)\n",
    "        torch.cuda.empty_cache()\n",
    "        # print training/validation statistics\n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "\n",
    "        epoch_len = len(str(epochs))\n",
    "\n",
    "        print_msg = (f'[{epoch:>{epoch_len}}/{epochs:>{epoch_len}}] ' +\n",
    "                     f'train_loss: {train_loss:.6f} ' +\n",
    "                     f'valid_loss: {valid_loss:.6f}')\n",
    "\n",
    "        print(print_msg)\n",
    "        # clear lists to track next epoch\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        pla_lr_scheduler.step(valid_loss)  # lr_scheduler\n",
    "        model_dict = {\n",
    "            'epoch': epoch,\n",
    "            'state_dict': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()\n",
    "        }\n",
    "        early_stopping(valid_loss.item(), model_dict, epoch, save_dir)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    with open(\"avg_train_losses.txt\", 'wt') as f:\n",
    "        for i in avg_train_losses:\n",
    "            print(i, file=f)\n",
    "\n",
    "    with open(\"avg_valid_losses.txt\", 'wt') as f:\n",
    "        for i in avg_valid_losses:\n",
    "            print(i, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "604dd72535c14f858f868784eaae8d96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=246.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "592dd6a578ed45689479466f864c9cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=246.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0/500] train_loss: 0.250081 valid_loss: 0.250058\n",
      "Validation loss decreased (inf --> 0.250058).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e87d4e0b024fdc9c47d600aa4f6c76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=246.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-35e90a2a810d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-39e0307224fa>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(net, device)\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_value_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m             t.set_postfix({\n\u001b[0;32m     62\u001b[0m                 \u001b[1;34m'trainloss'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'{:.6f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_aver\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\greg\\desktop\\standard_env\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\greg\\desktop\\standard_env\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m                 \u001b[1;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m                 \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(net, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analyse training metrics\n",
    "To view training loss in Tensorboard, enter ```tensorboard --logdir=<path_to_runs_dir>``` into the command line and navigate to ```localhost:6066/``` in your browser."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
